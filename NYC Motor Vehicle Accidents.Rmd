---
title: "NYPD Boroughs"
author: "Tarun Parmar"
date: "Feb 18, 2017"
output: html_notebook
---

## NYC Motor Vehicle Accidents

```{r, message=FALSE, warning=FALSE}
library(raster)
library(sparklyr)
library(dplyr)
library(ggplot2)
```

We'll first set up a connection to the Spark cluster and reference the table we want.

```{r}
sc <- spark_connect(master = "local", version = "2.0.1", hadoop_version = "2.7")
nypd <- spark_read_csv(sc, "nypd", "/home/rspark/myR/github/spark/NYPD_Motor_Vehicle_Collisions.csv", overwrite=TRUE)
```

A bit of data cleaning to ensure that we only get rows with that includes the fields that we're interested in.

```{r}
cleanNY <- nypd  %>% 
  filter(!is.na(LATITUDE), !is.na(LONGITUDE), LATITUDE != 0, LONGITUDE != 0, BOROUGH != "") %>% 
  arrange(desc(UNIQUE_KEY))

dplyr::explain(cleanNY)
```

Now that we have (the reference to) the table that we want, we can send it through an MLlib function to do some learning.

```{r}
sets <- sparklyr::sdf_partition(cleanNY, training=0.9, test = 0.1)
train <- sets$training
test <- sets$test

dt <- ml_decision_tree(train , "BOROUGH", c("LATITUDE", "LONGITUDE"), max.bins = 200L, max.depth=10L, seed=123L)

# numeric to string mapping
lbls <- dt$model.parameters$labels
boroughs <- data.frame(BID=0:(length(lbls)-1), BOROUGH=lbls)
btbl <- copy_to(sc, boroughs, overwrite = TRUE)

preds <- sdf_predict(dt, test) %>% left_join(btbl) %>% mutate(correct = prediction == BID) %>% left_join(btbl, by=c("prediction"="BID")) %>% mutate(BOROUGH=BOROUGH.x, PREDICTION_NAME = BOROUGH.y)
preds %>% group_by(correct) %>% summarize(count = n())
```

Since we want to plot the data locally, let's collect some of the data so we can get a decent plot to visualize our results. We'll just grab 100k rows. To be clear, this is not a limitation of Spark but rather is a limitation of a.) how much data we want to import into R and b.) how many points we want to try to plot.

```{r}
results <- preds %>% 
  select(BOROUGH, LATITUDE, LONGITUDE, PREDICTION_NAME, correct) %>% 
  collect()
```

We now have a local table that has 100k results with their predictions alongside them. We can plot out the true values for these 100k rows:

```{r}
results %>% 
  ggplot(aes(LONGITUDE, LATITUDE, color=as.factor(PREDICTION_NAME))) + 
    geom_point(alpha=0.2, size=1) + 
    guides(colour = guide_legend(override.aes = list(alpha = 1)))
```

Or we can plot where our MLlib prediction was in/correct:

```{r}
results %>% 
  ggplot(aes(LONGITUDE, LATITUDE, color=as.factor(correct))) + geom_point(alpha=0.2, size=1) + guides(colour = guide_legend(override.aes = list(alpha = 1)))
```

Pretty close! In total, we got `r (1 - (sum(!results$correct) / nrow(results))) * 100`% correct.

```{r}
# spark_disconnect(sc)
```